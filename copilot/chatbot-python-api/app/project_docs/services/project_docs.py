import asyncio
import re
import time

from app.core.config import MemorySize
from app.core.interfaces import IBaseLogger
from app.core.memory import MemoryManager
from app.core.pydantic_models import LLMModels, MessageRequest, MetricInput
from app.core.utils import extract_claim, extract_relevant_content
from app.core.prompt_manager import create_prompt_manager  # Added import
from app.metrics.rag_evaluator import RagEvaluator
from app.metrics.strategy_context import Context
from app.project_docs.services.ai_search import AISearch
from app.project_docs.services.followup_question import generate_followup_questions
from azure.search.documents.models import VectorizedQuery
from fastapi import HTTPException, status
from fastapi.encoders import jsonable_encoder
from llama_index.core.chat_engine import ContextChatEngine


class Service:
    """
    A class used to generate responses to questions about project documents.
    This class provides a method to execute the AI search and chat engine to generate a response
    """

    def __init__(self, logger: IBaseLogger) -> None:
        super().__init__()
        self._logger = logger
        # Initialize prompt manager using the factory function
        self._prompt_manager = create_prompt_manager(logger)
        # Log service initialization
        self._logger.info("Project Docs Service initialized", extra_dims={"component": "Project Docs", "service": "document_qa"})

    async def execute(
        self, message_request: MessageRequest, authorization: str, llm_models: LLMModels
    ) -> dict:
        """
        Generates a response to a question about project documents.
        This method uses the AI search and chat engine to generate a response to the question
        in the MessageRequest object. The response is returned as a dictionary.

        Args:
            message_request (MessageRequest): A MessageRequest object containing the question and documents IDs

        Returns:
            dict: A dictionary containing the response generated by the chat engine
        """
        # Track execution start time for performance metrics
        request_start_time = time.time()
        execution_metrics = {
            "component": "Project Docs",
            "operation": "document_qa",
            "chat_length": len(message_request.chatHistory) if message_request.chatHistory else 0,
            "question_length": len(message_request.question) if message_request.question else 0
        }
        
        # Enhanced request logging with truncated question for privacy/length
        question_preview = message_request.question[:50] + "..." if len(message_request.question) > 50 else message_request.question
        self._logger.info(
            f"Processing Project Docs request: '{question_preview}'", 
            extra_dims=execution_metrics
        )
        
        # Document validation
        documents = message_request.context.documents
        if not documents:
            self._logger.error("No document IDs provided in request", 
                              extra_dims={"error_type": "validation_error", "component": "Project Docs"})
            raise HTTPException(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                detail="No documents IDs in request payload",
            )
            
        # Log document information
        self._logger.info(
            f"Processing request with {len(documents)} documents", 
            extra_dims={"document_count": len(documents)}
        )

        # Generate embedding for query
        embedding_start = time.time()
        embedding = llm_models.embed_model.get_text_embedding(message_request.question)
        vector_query = VectorizedQuery(
            vector=embedding, k_nearest_neighbors=3, fields="chunk_embeddings"
        )
        embedding_time = round((time.time() - embedding_start) * 1000, 2)
        self._logger.info(
            f"Generated query embedding in {embedding_time}ms", 
            extra_dims={"embedding_time_ms": embedding_time}
        )
        self._logger.info("Starting project doc")

        # Token extraction and AI Search setup
        token_start = time.time()
        token = authorization[len("Bearer ") :]
        self._logger.info("Getting AI Search Instance Name and Index")
        index_name = extract_claim(token, "docs_index_name")
        aisearch_instance_name = extract_claim(token, "ai_search_instance_name")
        
        # Validate AI Search instance
        if not aisearch_instance_name:
            self._logger.error(
                "Invalid AI Search instance name in token", 
                extra_dims={"error_type": "token_validation", "missing_claim": "ai_search_instance_name"}
            )
            raise HTTPException(
                status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
                detail="Invalid AI Search instance name",
            )
            
        # Log extracted token information (masked for security)
        masked_instance = aisearch_instance_name[:4] + "..." if len(aisearch_instance_name) > 4 else "****"
        masked_index = index_name[:4] + "..." if index_name and len(index_name) > 4 else "****"
        token_time = round((time.time() - token_start) * 1000, 2)
        
        self._logger.info(
            f"Extracted search info from token in {token_time}ms", 
            extra_dims={
                "token_time_ms": token_time,
                "has_index_name": bool(index_name),
                "instance_name_preview": masked_instance,
                "index_name_preview": masked_index
            }
        )

        # Set up AI Search client
        search_setup_start = time.time()
        ai_search = AISearch(llm_models=llm_models)
        self._logger.info("Setting up AI Search client")
        await ai_search.async_set_client(index_name, aisearch_instance_name)
        search_setup_time = round((time.time() - search_setup_start) * 1000, 2)
        self._logger.info(
            f"AI Search client setup complete in {search_setup_time}ms", 
            extra_dims={"search_setup_time_ms": search_setup_time}
        )

        # Generate document filters
        filter_start = time.time()
        self._logger.info("Generating docs filters")

        search_in_start = "search.in(external_document_uuid, "
        document_ids = ",".join([str(doc) for doc in documents])
        filters = f"{search_in_start}'{document_ids}')"
        filter_time = round((time.time() - filter_start) * 1000, 2)
        self._logger.info(
            f"Document filters generated in {filter_time}ms", 
            extra_dims={"filter_time_ms": filter_time}
        )

        # Document retrieval
        retrieval_start = time.time()
        self._logger.info("Getting Retriever")
        retrieved_elements = await ai_search.get_documents(
            question=message_request.question,
            filters=filters,
            vector_query=vector_query,
            similarity_top_k=8,
            num_documents=len(documents),
        )
        
        # Log retrieval metrics
        chunk_count = len(retrieved_elements.get('rerieved_chunks', [])) if 'rerieved_chunks' in retrieved_elements else 0
        retrieval_time = round((time.time() - retrieval_start) * 1000, 2)
        self._logger.info(
            f"Retrieved {chunk_count} document chunks in {retrieval_time}ms", 
            extra_dims={
                "chunk_count": chunk_count,
                "retrieval_time_ms": retrieval_time
            }
        )
        
        # Memory generation
        memory_start = time.time()
        chat_history = MemoryManager.generate_chat_messages(
            message_request.chatHistory,  # type: ignore
            size=MemorySize.project_docs,
        )
        memory_time = round((time.time() - memory_start) * 1000, 2)
        self._logger.info(
            f"Generated chat history with {len(chat_history)} messages in {memory_time}ms", 
            extra_dims={
                "history_message_count": len(chat_history),
                "memory_time_ms": memory_time
            }
        )

        # Chat engine setup
        chat_setup_start = time.time()
        self._logger.info("Getting Chat Context")
        chat_engine_options = {
            "retriever": retrieved_elements.get('filtered_index'),
            "llm": llm_models.llm,
            "chat_history": chat_history,
        }
        
        # Process project description if available
        project_desc_start = time.time()
        project_description_context = ""
        if message_request.context.projectDescription:
            self._logger.info("Extracting relevant content from project description")
            project_description_context = extract_relevant_content(
                message_request.context.projectDescription,
                message_request.question,
                llm_models,
            )
            project_desc_length = len(project_description_context)
            original_length = len(message_request.context.projectDescription)
            compression_ratio = round(project_desc_length / original_length, 2) if original_length > 0 else 0
            
            self._logger.info(
                f"Extracted relevant project description ({project_desc_length} chars)", 
                extra_dims={
                    "original_length": original_length,
                    "extracted_length": project_desc_length,
                    "compression_ratio": compression_ratio,
                    "extraction_time_ms": round((time.time() - project_desc_start) * 1000, 2)
                }
            )

        # Get prompt templates
        prompt_start = time.time()
        self._logger.info("Retrieving prompt templates")
        
        # Get context template via prompt manager
        context_template = await self._prompt_manager.get_prompt(
            agent="project_docs",
            key="context_template",
            prompt_parameters={
                "additional_context": project_description_context,
            }
        )
        chat_engine_options |= {"context_template": context_template}
        
        # Get chat engine prompt via prompt manager
        chat_engine_prompt = await self._prompt_manager.get_prompt(
            agent="project_docs",
            key="chat_engine_context",
            prompt_parameters={
                "project_description": project_description_context,
            }
        )
        chat_engine_options |= {"system_prompt": chat_engine_prompt}
        
        prompt_time = round((time.time() - prompt_start) * 1000, 2)
        self._logger.info(
            f"Retrieved prompt templates in {prompt_time}ms", 
            extra_dims={"prompt_time_ms": prompt_time}
        )
        
        # Initialize chat engine
        chat_e = ContextChatEngine.from_defaults(**chat_engine_options)
        chat_setup_time = round((time.time() - chat_setup_start) * 1000, 2)
        self._logger.info(
            f"Chat engine setup complete in {chat_setup_time}ms", 
            extra_dims={"chat_setup_time_ms": chat_setup_time}
        )
        
        # Generate response
        response_start = time.time()
        payload = {"message": message_request.question}
        self._logger.info("Generating Response")
        raw_response = await chat_e.achat(**payload)
        response_time = round((time.time() - response_start) * 1000, 2)
        response_length = len(raw_response.response) if hasattr(raw_response, 'response') else 0
        
        self._logger.info(
            f"Generated response ({response_length} chars) in {response_time}ms", 
            extra_dims={
                "response_length": response_length,
                "response_time_ms": response_time
            }
        )
        
        # Prepare citing sources
        source_start = time.time()
        citing_sources = {
            "sourceName": "project-docs",
            "sourceType": "documents",
            "sourceValue": [],
        }

        # Calculate metrics and generate followup questions in parallel
        metrics_start = time.time()
        self._logger.info("Calculating metrics and generating followup questions")
        metric_input = MetricInput(
            user_input=message_request.question,
            llm_response=raw_response.response,
            retrieved_context=retrieved_elements.get('rerieved_chunks'),
        )
        self._logger.info("Calculating score")
        current_metric = Context(RagEvaluator())
        tasks = [
            current_metric.run(metric_input),
            generate_followup_questions(
                message_request.question,
                llm_models,
                chat_history,
                retrieved_elements.get("rerieved_chunks"),
            ),
        ]
        
        # Execute metric and followup tasks concurrently
        results = await asyncio.gather(*tasks)
        score = results[0]
        followup_questions = results[1]
        # We cannot have duplicated citing sources, and this is happening,
        # probably due to the top_k used. Therefore, we need to remove the duplicated
        # citing sources. We accomplish is by keeping track chunks IDs and only
        # using that chunk to build the citing sources if it is the first time that
        # has seen
        seen_chunk_ids = set()
        nodes_metadata = []
        for node in raw_response.source_nodes:
            # nodes can be empty without chunk_id
            chunk_id = node.metadata.get("chunk_id", None)
            if chunk_id and chunk_id not in seen_chunk_ids:
                pattern = r"END for Page Number - (\d+)"
                matches = re.findall(pattern, node.text)
                pages = [int(match) for match in matches]
                node.metadata["pages"] = pages
                node.metadata["chunk_text"] = node.text
                nodes_metadata.append(node.metadata)
                seen_chunk_ids.add(chunk_id)
                
        citing_sources["sourceValue"] = nodes_metadata
        source_time = round((time.time() - source_start) * 1000, 2)
        
        self._logger.info(
            f"Processed {len(nodes_metadata)} source nodes in {source_time}ms", 
            extra_dims={
                "source_node_count": len(nodes_metadata),
                "unique_chunk_count": len(seen_chunk_ids),
                "source_time_ms": source_time
            }
        )
        
        # Prepare final response
        response_prep_start = time.time()
        response = {
            "backend": "project-docs",
            "status_code": 200,
            "chainOfThoughts": ""
        }
        
        if nodes_metadata:
            self._logger.info("Updating Response with Citing Sources")
            response.update(
                {
                    "response": raw_response.response,
                    "citingSources": [citing_sources],
                    "rawResponse": raw_response,
                    "score": score,
                    "followUpSuggestions": followup_questions,
                }
            )
        else:
            self._logger.info(
                "No valid sources found, returning no content response", 
                extra_dims={"response_type": "no_content"}
            )
            response.update({"status_code": 204})
            
        response_prep_time = round((time.time() - response_prep_start) * 1000, 2)
        self._logger.info(
            f"Response preparation completed in {response_prep_time}ms", 
            extra_dims={"response_prep_time_ms": response_prep_time}
        )
        
        # Log total execution time
        total_time = round((time.time() - request_start_time) * 1000, 2)
        self._logger.log_metric(
            "total_execution_time_ms", 
            total_time,
            extra_dims={
                "response_type": "with_sources" if nodes_metadata else "no_content",
                "has_score": score is not None,
                "has_followup": followup_questions is not None and len(followup_questions) > 0
            }
        )
        
        self._logger.info(
            f"Project Docs execution completed in {total_time}ms", 
            extra_dims={
                "total_execution_time_ms": total_time,
                "response_type": "with_sources" if nodes_metadata else "no_content"
            }
        )
        
        encoded_response = jsonable_encoder(response)
        return encoded_response
